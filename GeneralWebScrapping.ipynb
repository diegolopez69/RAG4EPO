{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lapto's headers: https://httpbin.org/anything\n",
    "headers =  {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\", \n",
    "    #\"Accept-Encoding\": \"gzip, deflate, br, zstd\", \n",
    "    \"Accept-Language\": \"en-US,en;q=0.9,es;q=0.8\", \n",
    "    \"Dnt\": \"1\", \n",
    "    #\"Host\": \"httpbin.org\", \n",
    "    \"Referer\": \"https://www.scraperapi.com/\", \n",
    "    \"Sec-Ch-Ua\": \"\\\"Google Chrome\\\";v=\\\"125\\\", \\\"Chromium\\\";v=\\\"125\\\", \\\"Not.A/Brand\\\";v=\\\"24\\\"\", \n",
    "    \"Sec-Ch-Ua-Mobile\": \"?0\", \n",
    "    \"Sec-Ch-Ua-Platform\": \"\\\"Windows\\\"\", \n",
    "    \"Sec-Fetch-Dest\": \"document\", \n",
    "    \"Sec-Fetch-Mode\": \"navigate\", \n",
    "    \"Sec-Fetch-Site\": \"cross-site\", \n",
    "    \"Sec-Fetch-User\": \"?1\", \n",
    "    \"Upgrade-Insecure-Requests\": \"1\", \n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\", \n",
    "    \"X-Amzn-Trace-Id\": \"Root=1-665ec732-28d2172a4721ed726fc59ceb\"\n",
    "  }\n",
    "\n",
    "def get_page(an):\n",
    "    URL = f\"https://register.epo.org/smartSearch?query={an}\" #https://register.epo.org/smartSearch?query= #https://register.epo.org/application?number=EP\n",
    "    page = requests.get(URL,headers=headers)\n",
    "    return page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_tabla(soup):\n",
    "    tablas = soup.find_all('table')\n",
    "    return tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Patent                                        Patent_Name  \\\n",
      "0  EP1125359  METHOD AND SYSTEM FOR DOWN-CONVERTING AN ELECT...   \n",
      "\n",
      "                                  Status Most_Recent_Event_Date  \\\n",
      "0  No opposition filed within time limit             28.03.2008   \n",
      "\n",
      "                                   Most_Recent_Event  \\\n",
      "0  Lapse of the patent in a contracting stateNew ...   \n",
      "\n",
      "         Divisional_applications  \\\n",
      "0  {\"EP02079190.1\": \"EP1315285\"}   \n",
      "\n",
      "                                           Inventors  \\\n",
      "0  [{\"number\": \"01\", \"name\": \"SORRELLS, David, F....   \n",
      "\n",
      "                                        Publications  \n",
      "0  [{\"type\": \"A1 Application with search report\",...  \n"
     ]
    }
   ],
   "source": [
    "def extraer_valores(texto):\n",
    "    # Extraer número de patente\n",
    "    patent = re.findall(r'<span class=\"highlight\">EP<span class=\"highlight\">(\\d+)</span></span>', texto)\n",
    "    if len(patent) == 0:\n",
    "        patent = ''\n",
    "    else:\n",
    "        patent = 'EP' + patent[0]\n",
    "\n",
    "    # Extraer nombre de la patente\n",
    "    patent_name = re.findall(r'</span></span> - (.*?)</a>', texto)\n",
    "    if len(patent_name) == 0:\n",
    "        patent_name = ''\n",
    "    else:\n",
    "        patent_name = patent_name[0].strip()  \n",
    "\n",
    "    # Extraer Status\n",
    "    status = re.findall(\"\"\"Status</td><td class=\"t2\" colspan=\"3\">.*?<br/>\"\"\", texto)\n",
    "    if len(status) == 0:\n",
    "        status = ''\n",
    "    else:\n",
    "        status = status[0]\n",
    "        status = status.replace(\"\"\"Status</td><td class=\"t2\" colspan=\"3\">\"\"\",\"\").replace('<br/>','').strip()\n",
    "\n",
    "    #Most Recent Event date\n",
    "    most_recent_event_date = re.findall(r'Most recent event.*?<td class=\"t2\">(.*?)</td>', texto, re.DOTALL)\n",
    "    if len(most_recent_event_date) == 0:\n",
    "        most_recent_event_date = ''\n",
    "    else:\n",
    "        most_recent_event_date = most_recent_event_date[0].strip()\n",
    "\n",
    "    #Most recent event\n",
    "    most_recent_event = re.findall(r'Most recent event.*?<td class=\"t3\">(.*?)</td>', texto, re.DOTALL)\n",
    "    if len(most_recent_event) == 0:\n",
    "        most_recent_event = ''\n",
    "    else:\n",
    "        most_recent_event = most_recent_event[0].replace('<br/>', '').replace('\\r\\n', '').replace('\\t', '').replace('\\xa0', ' ').strip()\n",
    "\n",
    "    #Divisional Applications\n",
    "    divisional_apps = re.findall(r'<td class=\"t2\" colspan=\"3\">(EP\\d+\\.\\d+)\\s*\\/\\s*<a.*?>(EP\\d+)</a>', texto)\n",
    "    divisional_dict = dict(divisional_apps)\n",
    "\n",
    "    # Extraer inventores\n",
    "    inventors = re.findall(r'<td class=\"t2\" colspan=\"3\">(\\d+)\\xa0/\\r\\n\\t(.*?)<br/>\\r\\n\\t(.*?)<br/>\\r\\n\\t(.*?)\\r\\n\\t  / (.*?)<br/>', texto)\n",
    "    inventors_dict = []\n",
    "    for inv in inventors:\n",
    "        inventor_info = {\n",
    "            'number': inv[0],\n",
    "            'name': inv[1],\n",
    "            'address': inv[2] + ', ' + inv[3],\n",
    "            'country': inv[4]\n",
    "        }\n",
    "        inventors_dict.append(inventor_info)\n",
    "\n",
    "    # Extraer publicaciones \n",
    "    publications = re.findall(r'<td class=\"th\">Type:\\r\\n\\s*</td>\\s*<td class=\"t2\" colspan=\"2\">(.*?)</td>\\s*<td class=\"th\">No.:</td>\\s*<td class=\"t2\" colspan=\"2\">(.*?)</td>\\s*<td class=\"th\">Date:</td>\\s*<td class=\"t2\" colspan=\"2\">(.*?)</td>\\s*<td class=\"th\">Language:</td>\\s*<td class=\"t2\" colspan=\"2\">(.*?)</td>', texto)\n",
    "    publications_list = []\n",
    "    for pub in publications:\n",
    "        # Extraer el texto del enlace si existe, o usar el texto plano si no hay enlace\n",
    "        type_text = re.search(r'>([^<]+)</a>$', pub[0])\n",
    "        if type_text:\n",
    "            pub_type = type_text.group(1).strip()\n",
    "        else:\n",
    "            pub_type = re.sub(r'\\xa0', ' ', pub[0]).strip()\n",
    "        publication_info = {\n",
    "            'type': pub_type,\n",
    "            'number': re.sub(r'<.*?>', '', pub[1]).strip(),\n",
    "            'date': pub[2].strip(),\n",
    "            'language': pub[3].strip()\n",
    "        }\n",
    "        publications_list.append(publication_info)\n",
    "    \n",
    "    # \"\"\"\n",
    "    # # Extraer Priority numbers\n",
    "    # #TODO: Extraer los priority numbers\n",
    "    # priority_section = re.search(r'<td class=\"th\" rowspan=\"\\d+\">Priority number, date</td>(.*?)<td class=\"t2\" colspan=\"3\">\\[', texto, re.DOTALL)\n",
    "    # if not priority_section:\n",
    "    #     return []\n",
    "    # priority_data = priority_section.group(1)\n",
    "    \n",
    "    # priority_numbers = re.findall(r'<td class=\"t2\">([A-Z]+\\d+)</td><td class=\"t3\" colspan=\"2\">(.*?)\\s*Original published format:\\s*(.*?)</td>', priority_data)\n",
    "    # priority_list = []\n",
    "    # for prio in priority_numbers:\n",
    "    #     priority_info = {\n",
    "    #         'number': prio[0],\n",
    "    #         'date': prio[1].strip(),\n",
    "    #         'original_format': prio[2].strip()\n",
    "    #     }\n",
    "    #     priority_list.append(priority_info)\n",
    "\n",
    "    # #Opponents\n",
    "    # #TODO: Extraer los priority numbers\n",
    "    # oponentes_actuales = re.findall(r'<td class=\"t2\" colspan=\"2\">(\\d+)\\s*([\\d.]+)\\s*([\\d.]+)\\s*ADMISSIBLE<br/>(.*?)<br/>(.*?)<br/>(.*?)\\s*/\\s*(.*?)<br/>Opponent\\'s representative<br/>(.*?)<br/>(.*?)<br/>(.*?)<br/>(.*?)<br/>(.*?)\\s*/\\s*(.*?)<br/>', texto)\n",
    "    # print(oponentes_actuales)\n",
    "    # oponentes_actuales_list = []\n",
    "    # for oponente in oponentes_actuales:\n",
    "    #     oponente_info = {\n",
    "    #         'number': oponente[0],\n",
    "    #         'date1': oponente[1],\n",
    "    #         'date2': oponente[2],\n",
    "    #         'name': oponente[3],\n",
    "    #         'address': f\"{oponente[4]}, {oponente[5]}\",\n",
    "    #         'country': oponente[6],\n",
    "    #         'representative': oponente[7],\n",
    "    #         'rep_address': f\"{oponente[8]}, {oponente[9]}, {oponente[10]}, {oponente[11]}\",\n",
    "    #         'rep_country': oponente[12]\n",
    "    #     }\n",
    "    #     oponentes_actuales_list.append(oponente_info)\n",
    "\n",
    "    # # Extraer oponentes anteriores (former)\n",
    "    # oponentes_former = re.findall(r'<tr class=\"former\">.*?Former \\[(.*?)\\].*?<td class=\"t2\" colspan=\"2\">(\\d+)\\s*([\\d.]+)\\s*([\\d.]+)\\s*ADMISSIBLE<br/>(.*?)<br/>(.*?)<br/>(.*?)\\s*/\\s*(.*?)<br/>Opponent\\'s representative<br/>(.*?)<br/>(.*?)<br/>(.*?)<br/>(.*?)<br/>(.*?)\\s*/\\s*(.*?)<br/>', texto, re.DOTALL)\n",
    "    \n",
    "    # oponentes_former_list = []\n",
    "    # for oponente in oponentes_former:\n",
    "    #     oponente_info = {\n",
    "    #         'version': oponente[0],\n",
    "    #         'number': oponente[1],\n",
    "    #         'date1': oponente[2],\n",
    "    #         'date2': oponente[3],\n",
    "    #         'name': oponente[4],\n",
    "    #         'address': f\"{oponente[5]}, {oponente[6]}\",\n",
    "    #         'country': oponente[7],\n",
    "    #         'representative': oponente[8],\n",
    "    #         'rep_address': f\"{oponente[9]}, {oponente[10]}, {oponente[11]}, {oponente[12]}\",\n",
    "    #         'rep_country': oponente[13]\n",
    "    #     }\n",
    "    #     oponentes_former_list.append(oponente_info)\n",
    "    # \"\"\"\n",
    "\n",
    "    # Crear DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Patent': [patent],\n",
    "        'Patent_Name': [patent_name],\n",
    "        'Status': [status],\n",
    "        'Most_Recent_Event_Date': [most_recent_event_date],\n",
    "        'Most_Recent_Event': [most_recent_event],\n",
    "        'Divisional_applications': [json.dumps(divisional_dict)],\n",
    "        'Inventors': [json.dumps(inventors_dict)],\n",
    "        'Publications': [json.dumps(publications_list)]\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "htmlContent = get_page(1125359)\n",
    "if(htmlContent and \"Just a moment...\" not in htmlContent):\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "    mytable = extraer_tabla(soup)[0]\n",
    "    texto_limpio = extraer_valores(str(mytable))\n",
    "    print(texto_limpio)\n",
    "else:\n",
    "    print(\"WebScrapping no disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Patent                                        Patent_Name  \\\n",
      "0  EP1999020  STRUCTURE FOR AN AIR INLET LIP FOR A NACELLE W...   \n",
      "\n",
      "                                  Status Most_Recent_Event_Date  \\\n",
      "0  No opposition filed within time limit             15.07.2016   \n",
      "\n",
      "                                   Most_Recent_Event Divisional_applications  \\\n",
      "0  Lapse of the patent in a contracting stateNew ...                      {}   \n",
      "\n",
      "                                           Inventors  \\\n",
      "0  [{\"number\": \"01\", \"name\": \"VAUCHEL, Guy, Berna...   \n",
      "\n",
      "                                        Publications  \n",
      "0  [{\"type\": \"A1 Application with search report\",...  \n"
     ]
    }
   ],
   "source": [
    "htmlContent = get_page(1999020)\n",
    "if(htmlContent and \"Just a moment...\" not in htmlContent):\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "    mytable = extraer_tabla(soup)[0]\n",
    "    texto_limpio = extraer_valores(str(mytable))\n",
    "    print(texto_limpio)\n",
    "else:\n",
    "    print(\"WebScrapping no disponible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Patent                                        Patent_Name  \\\n",
      "0  EP1492505  PHARMACEUTICAL PREPARATION CONTAINING OXYCODON...   \n",
      "\n",
      "           Status Most_Recent_Event_Date  \\\n",
      "0  Patent revoked             26.05.2017   \n",
      "\n",
      "                            Most_Recent_Event  \\\n",
      "0  Lapse of the patent in a contracting state   \n",
      "\n",
      "                             Divisional_applications  \\\n",
      "0  {\"EP10176720.0\": \"EP2319496\", \"EP11177513.6\": ...   \n",
      "\n",
      "                                           Inventors  \\\n",
      "0  [{\"number\": \"01\", \"name\": \"BR\\u00d6GMANN, Bian...   \n",
      "\n",
      "                                        Publications  \n",
      "0  [{\"type\": \"A2 Application without search repor...  \n"
     ]
    }
   ],
   "source": [
    "htmlContent = get_page(1492505)\n",
    "if(htmlContent and \"Just a moment...\" not in htmlContent):\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "    mytable = extraer_tabla(soup)[0]\n",
    "    texto_limpio = extraer_valores(str(mytable))\n",
    "    print(texto_limpio)\n",
    "else:\n",
    "    print(\"WebScrapping no disponible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EP10176720.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_str = texto_limpio['Divisional_applications'][0]\n",
    "\n",
    "# Convertir la cadena JSON en un diccionario de Python\n",
    "json_dict = json.loads(json_str)\n",
    "\n",
    "# Obtener el primer key del diccionario\n",
    "primer_key = list(json_dict.keys())[0]\n",
    "\n",
    "primer_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá comienzo a unir los df's para que ya se haga todo en conjunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv('/Users/diego/Documents/Universidades/Complutense/TFM/Sandov/DatosDivisionals20110101_20110630.csv')\n",
    "# df2 = pd.read_csv('/Users/diego/Documents/Universidades/Complutense/TFM/Sandov/DatosDivisionals20110701_20111231.csv')\n",
    "# df3 = pd.read_csv('/Users/diego/Documents/Universidades/Complutense/TFM/Sandov/DatosDivisionals20120101_20120630.csv')\n",
    "# df4 = pd.read_csv('/Users/diego/Documents/Universidades/Complutense/TFM/Sandov/DatosDivisionals20120701_20121231.csv')\n",
    "# df5 = pd.read_csv('/Users/diego/Documents/Universidades/Complutense/TFM/Sandov/DatosDivisionals20130101_20130631.csv')\n",
    "# df6 = pd.read_csv('/Users/diego/Documents/Universidades/Complutense/TFM/Sandov/DatosDivisionals20130601_20131231.csv')\n",
    "# df7 = pd.read_csv('/Users/diego/Documents/Universidades/Complutense/TFM/Sandov/DatosDivisionals20140101_20140631.csv')\n",
    "# df8 = pd.read_csv('/Users/diego/Documents/Universidades/Complutense/TFM/Sandov/DatosDivisionals20140601_20141231.csv')\n",
    "# dfAll = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8], axis=0, ignore_index=True)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/Users/diego/Documents/Universidades/Complutense/TFM/Sandov/Datos/Patents20090101_20090531.csv')\n",
    "df2 = pd.read_csv('/Users/diego/Documents/Universidades/Complutense/TFM/Sandov/Datos/Patents20090531_20091231.csv')\n",
    "dfAll = pd.concat([df1, df2], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>family</th>\n",
       "      <th>publication_number</th>\n",
       "      <th>application_number</th>\n",
       "      <th>ipc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>89987512</td>\n",
       "      <td>P0700339</td>\n",
       "      <td>P0700339</td>\n",
       "      <td>A61P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35394157</td>\n",
       "      <td>P0700044</td>\n",
       "      <td>P0700044</td>\n",
       "      <td>A61P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>89987139</td>\n",
       "      <td>P0600847</td>\n",
       "      <td>P0600847</td>\n",
       "      <td>A61P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>89985663</td>\n",
       "      <td>P0402490</td>\n",
       "      <td>P0402490</td>\n",
       "      <td>A61P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>26805159</td>\n",
       "      <td>09C0004</td>\n",
       "      <td>09C0004</td>\n",
       "      <td>A61P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19229</th>\n",
       "      <td>9900</td>\n",
       "      <td>35613661</td>\n",
       "      <td>88018</td>\n",
       "      <td>A200702125</td>\n",
       "      <td>A61K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19230</th>\n",
       "      <td>9901</td>\n",
       "      <td>34979083</td>\n",
       "      <td>88012</td>\n",
       "      <td>A200612978</td>\n",
       "      <td>A61K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19231</th>\n",
       "      <td>9902</td>\n",
       "      <td>35124997</td>\n",
       "      <td>88008</td>\n",
       "      <td>A200611517</td>\n",
       "      <td>A61K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19232</th>\n",
       "      <td>9903</td>\n",
       "      <td>32322658</td>\n",
       "      <td>88002</td>\n",
       "      <td>A200610767</td>\n",
       "      <td>A61K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19233</th>\n",
       "      <td>9904</td>\n",
       "      <td>34115512</td>\n",
       "      <td>87991</td>\n",
       "      <td>A200605346</td>\n",
       "      <td>A61K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19234 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0    family publication_number application_number   ipc\n",
       "0               0  89987512           P0700339           P0700339  A61P\n",
       "1               1  35394157           P0700044           P0700044  A61P\n",
       "2               2  89987139           P0600847           P0600847  A61P\n",
       "3               3  89985663           P0402490           P0402490  A61P\n",
       "4               4  26805159            09C0004            09C0004  A61P\n",
       "...           ...       ...                ...                ...   ...\n",
       "19229        9900  35613661              88018         A200702125  A61K\n",
       "19230        9901  34979083              88012         A200612978  A61K\n",
       "19231        9902  35124997              88008         A200611517  A61K\n",
       "19232        9903  32322658              88002         A200610767  A61K\n",
       "19233        9904  34115512              87991         A200605346  A61K\n",
       "\n",
       "[19234 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll = dfAll.drop_duplicates()\n",
    "dfAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P0700339\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m htmlContent \u001b[38;5;241m=\u001b[39m get_page(codigo)\n\u001b[1;32m      7\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(htmlContent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m mytable \u001b[38;5;241m=\u001b[39m \u001b[43mextraer_tabla\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoup\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m texto_limpio \u001b[38;5;241m=\u001b[39m extraer_valores(\u001b[38;5;28mstr\u001b[39m(mytable))\n\u001b[1;32m     10\u001b[0m dfs\u001b[38;5;241m.\u001b[39mappend(texto_limpio)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "for index, row in dfAll.iterrows():\n",
    "    codigo = row['publication_number']\n",
    "    print(codigo)\n",
    "    htmlContent = get_page(codigo)\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "    mytable = extraer_tabla(soup)[0]\n",
    "    texto_limpio = extraer_valores(str(mytable))\n",
    "    dfs.append(texto_limpio)\n",
    "    time.sleep(15)#Para no sobrecargar la api\n",
    "\n",
    "resultado_final = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_final.to_csv(\"/Users/diego/Documents/Universidades/Complutense/TFM/Sandov/DatosDFFinal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a_iterar = pd.read_csv('/Users/diego/Documents/Universidades/Complutense/TFM/Sandov/DatosDFFinal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a_iterar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a_iterar['Divisional_applications'] = df_a_iterar['Divisional_applications'].apply(literal_eval)\n",
    "\n",
    "# Ahora, extraemos los valores de los diccionarios\n",
    "nuevo_df = pd.DataFrame({    \n",
    "    'publication_number': df_a_iterar['Divisional_applications'].apply(lambda x: list(x.values()))\n",
    "})\n",
    "\n",
    "# Expandimos la lista de valores en columnas separadas\n",
    "nuevo_df = nuevo_df.explode('publication_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_df = nuevo_df.dropna()\n",
    "nuevo_df = nuevo_df.reset_index(drop=True)\n",
    "nuevo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos generado una nueva lista en base a los Publication number obtenidos del Scrappeo, volvemos a iterar sobre estos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for index, row in nuevo_df.iterrows():\n",
    "    codigo = row['publication_number']\n",
    "    print(codigo)\n",
    "    htmlContent = get_page(codigo)\n",
    "    soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "    mytable = extraer_tabla(soup)[0]\n",
    "    texto_limpio = extraer_valores(str(mytable))\n",
    "    dfs.append(texto_limpio)\n",
    "    time.sleep(15)#Para no sobrecargar la api\n",
    "\n",
    "resultado_final_segunda_iteracion = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_final_segunda_iteracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exterior = pd.DataFrame(columns=['publication_number'])\n",
    "\n",
    "df_inicial = pd.read_csv('/Users/diego/Documents/Universidades/Complutense/TFM/Sandov/DatosDFFinal.csv')\n",
    "\n",
    "def procesar_df(df):\n",
    "    df['Divisional_applications'] = df['Divisional_applications'].apply(literal_eval)\n",
    "    nuevo_df = pd.DataFrame({\n",
    "        'publication_number': df['Divisional_applications'].apply(lambda x: list(x.values()))\n",
    "    })\n",
    "    nuevo_df = nuevo_df.explode('publication_number')\n",
    "    nuevo_df = nuevo_df.dropna()\n",
    "    nuevo_df = nuevo_df.reset_index(drop=True)\n",
    "    return nuevo_df\n",
    "\n",
    "df_actual = df_inicial\n",
    "\n",
    "for iteracion in range(5):\n",
    "    print(f\"Iniciando iteración {iteracion + 1}\")\n",
    "    \n",
    "    df_procesado = procesar_df(df_actual)\n",
    "    \n",
    "    df_exterior = pd.concat([df_exterior, df_procesado[['publication_number']]], ignore_index=True)\n",
    "    \n",
    "    dfs = []\n",
    "    for index, row in df_procesado.iterrows():\n",
    "        codigo = row['publication_number']\n",
    "        print(f\"Procesando código: {codigo}\")\n",
    "        htmlContent = get_page(codigo)\n",
    "        soup = BeautifulSoup(htmlContent, 'html.parser')\n",
    "        mytable = extraer_tabla(soup)[0]\n",
    "        texto_limpio = extraer_valores(str(mytable))\n",
    "        dfs.append(texto_limpio)\n",
    "        time.sleep(15)  # Para no sobrecargar la api\n",
    "    \n",
    "    resultado_iteracion = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    if 'Divisional_applications' in resultado_iteracion.columns and not resultado_iteracion['Divisional_applications'].isna().all():\n",
    "        df_actual = resultado_iteracion\n",
    "    else:\n",
    "        print(f\"No hay más aplicaciones divisionales después de la iteración {iteracion + 1}\")\n",
    "        break\n",
    "\n",
    "    print(f\"Finalizada iteración {iteracion + 1}\")\n",
    "\n",
    "df_exterior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
